# AIML-Projects
![Image](https://www.simplilearn.com/ice9/free_resources_article_thumb/Cutting_Edge_Innovative_AI_Project_Ideas.jpg)
---
This is a collection of the different Machine Learning and Deep Learning Projects undertaken by me.  Some of these projects are part of the PGP AI-ML course from Great Lakes Institute of Management, that I undertook from Nov' 19 to May' 21. 

I have collected all the different projects, assessments & other research ideas that I have either been assigned to during my PGP Course or 
have come across while exploring different nuances of my academic journey.

Some of these projects have been performed on curated datasets, while some have been extracted from the internet. Some projects have been an extension
of existing projects & I have tried to improve the performance of the same, while have been on fresh datasets where I have tried to achieve good results.

All projects have associated explanations & the idea behind executing the same.

Do reach out to me in case of any queries or suggestions on further improving my work. Cheerio :-)

### Index
|__Problem__|__Methods__|__Libs__|__Repo__|
|-|-|-|-|
|[Generating YT Video Summary and Quiz sessions from Transcripts](#YT-Video-Summarization-Translation)|`Using LLMs for Summarization & Translation` | `Google Colab`, `OpenAI API`, `HuggingFace`, `Transformers`, `LLMs` | [Click](https://github.com/debajyotid/Generating_YT_Video_Summary_and_Quiz_sessions_from_Transcripts)|
|[Deploying Machine Learning Models on different Cloud Frameworks](#ML-Model-deployment-on-cloud)|`Model Deployment` | `Heroku`, `AWS EC2`, `Azure`, `Postman`, `StreamLit`, `FastAPI`|
|[Identifying Dog Breeds based on Visual Variations in appearance, using Convolution Neural Networks & Transfer Learning](#Using-CNN-to-classify-Dog-Breeds-based-on-images)|`Capstone Project` | `CNN`, `VGG16`, `ResNet50`, `MobileNetV2`, `Image Augmentation`, `Transfer Learning`|[Click](https://github.com/debajyotid/Using-CNN-to-classify-Dog-Breeds-based-on-images)|
|[Using Siamese Networks to distinguish a Forged Signature from Genuine Signature](#Signature-Verification-using-Siamese-Networks)|`Advanced Computer Vision` | `CNN`, `Siamese Network`, `Contrastive Loss`, `Model Checkpoint`, `Signature Verification`|[Click](https://github.com/debajyotid/Signature-Verification-using-Siamese-Networks)|
|[Using Glove and LSTMs to classify Fake News](#Using-Glove-and-LSTMs-to-classify-Fake-News)|`Sequential NLP` | `LSTM`, `Glove Word Embeddings`, `Keras Tokenization`, `Kaggle`, `Stemming`|[Click](https://github.com/debajyotid/Using-Glove-and-LSTMs-to-classify-Fake-News/blob/main/NLP-Kaggle%20Fake%20News%20Classifier%20using%20Glove%20%26%20LSTM.ipynb)|
|[Face Recognition model using VGG](#Alligned-Face-Dataset-Pinterest)|`Face Recognition` | `VGG`, `SVM`, `Embeddings`, `Similarity Distance`, `OpenCV`, `PCA`, `StandardScaler`, `LabelEncoder`,|[Click](https://github.com/debajyotid/Creating-a-Face-Recognition-Model-using-a-VGG-Architecture-and-pretrained-weights/blob/main/Creating%20a%20Face%20Recognition%20Model%20using%20a%20VGG%20Architecture%20and%20pretrained%20weights.ipynb)|
|[Using Mobilenet in UNet Architecture to predict Face Masks](#WIDER-Face-Dataset)|`Face Detection` | `UNet`,`MobileNet`,`CV2`,`Dice-Coefficient`,`Callbacks`,`ReduceLROnPlateau`,`EarlyStopping`,`ModelCheckpoint`,`Learning Plots`|[Click](https://github.com/debajyotid/Using-MobileNet-in-UNet-Architecture-to-build-a-Face-Mask-Prediction-Model/blob/main/Using%20MobileNet%20in%20UNet%20Architecture%20to%20build%20a%20Face%20Mask%20Prediction%20Model.ipynb)|
|[Using Statistical NLP to predict author of Blog](#Kaggle-Blog-Authoship-Corpus-Challenge)|`Statistical NLP` | `NLTK`,`WordNetLemmatizer`,`CountVectorizer`,`OneVsRestClassifier`,`LogisticRegression`,`MultiLabelBinarizer`|[Click](https://github.com/debajyotid/Great-Lakes/blob/master/Deep%20Learning/Using%20Statistical%20NLP%20to%20predict%20author%20of%20Blog.ipynb)|
|[Using Computer Vision and Transfer Learning to distinguish between Original and Fake Images](#Fake-vs-Real-Image-Detection-Challenge)|`Image Detection` | `Resnet50`,`Computer Vision`,`Image Augmentation`|[Click](https://github.com/debajyotid/Fake-vs-Real-Image-Detection-Challenge)|
|[Using AWS SageMaker to predict Bike Sharing Demand](#Predict-Bike-Sharing-Demand-using-AWS-SageMaker)|`Model Deployment on Cloud` | `SageMaker`,`XGBoost`,`S3 Buckets`|[Click](https://github.com/debajyotid/SageMaker-Bike-Sharing)|
|[Using NLP to classify Rotten Tomato Reviews into Sentiments](#Sentiment-Analysis-on-Movie-Reviews-Classify-the-sentiment-of-sentences-from-the-Rotten-Tomatoes-dataset)|`Natural Language Processing` | `CountVectorizer`,`Tf-Idf Vectorizer`,`Glove-Word Embeddings`,`Word2Vec`,`Neural Networks`,`matplotlib.pyplot`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Rotten-Tomatoes-review-Sentiment-Analysis/blob/main/Rotten%20Tomatoes%20review%20Sentiment%20Analysis.ipynb)|
|[Using NLP Techniques to analyze IMDB Movie Reviews Sentiment](#Bag-of-Words-Meets-Bags-of-Popcorn-Using-Google-Word2Vec-for-movie-reviews)|`Natural Language Processing` | `CountVectorizer`,`Tf-Idf Vectorizer`,`Glove-Word Embeddings`,`Word2Vec`,`Neural Networks`,`matplotlib.pyplot`|[Click](https://nbviewer.jupyter.org/github/debajyotid/IMDB-Reviews-Sentiment-Analysis-using-NLP-Technques/blob/main/IMDB%20Reviews%20Sentiment%20Analysis%20using%20NLP%20Technques.ipynb)|
|[Analysing the impact of lockdown in curtailing the spread of COVID19 in Germany](#Determining-the-preparedness-of-the-German-Government-and-Medical-Authorities-in-handling-COVID19-Crisis)|`Statistical Methods` | `matplotlib.pyplot`, `scipy.stats`,  `sklearn.preprocessing`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Mini%20Capstone/Covid-19%20Germany%20(Data%20Pre%20Processing%20%26%20Hypothesis%20Testing)_Final%20version.ipynb)|
|[Executing XGBoost & LightGBM on same dataset to compare the same](#Understanding-the-performance-of-LightGBM-and-XGBoost)|`Ensemble Techniques` | `XGBoost`, `LightGBM`,  `pca`|[Click](https://github.com/debajyotid/Understanding-the-performance-of-LightGBM-and-XGBoost/blob/main/Understanding%20the%20performance%20of%20LightGBM%20and%20XGBoost.ipynb)|
|[Imbalanced Dataset handling and PCA to reduce multi-collinearity](#Understanding-impact-of-Imblearn-and-PCA)|`Imbalanced Learning-PCA` | `sklearn.model_selection`, `sklearn.preprocessing`,  `imblearn`, `RandomForestClassifier`, `sklearn.ensemble`,`pca`|[Click](https://github.com/debajyotid/Understanding-impact-of-Imblearn-and-PCA/blob/main/Understanding%20impact%20of%20Imblearn%20and%20PCA.ipynb)|
|[Predicting Salary Range using ML Techniques](#Predicting-Salary-Range-using-ML-Techniques)|`Ensemble Techniques` | `sklearn.model_selection`, `sklearn.preprocessing`,  `imblearn`, `xgboost`, `sklearn.ensemble`,`mlxtend.classifier`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Predicting-Salary-Range-using-ML-Techniques/blob/master/Predicting_SalaryRange_using_MLTechniques.ipynb)|
|[Predicting Mutual Fund Ratings](#Using-ML-techniques-to-predict-GreatStone-Ratings-for-MutualFunds)|`Ensemble Techniques` | `sklearn.model_selection`, `sklearn.preprocessing`,  `imblearn`, `sklearn.svm`, `xgboost`, `sklearn.ensemble`,`mlxtend.classifier`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Predicting-Mutual-Fund-Ratings/blob/master/Predicting_MutualFund_Ratings_using_ML_techniques.ipynb)|
|[Using CNN to classify German Traffic Signs](#Using-CNN-to-classify-German-Traffic-Signs)|`Convolutional Neural Networks` | `tensorflow.keras`, `tensorflow.keras.preprocessing.image`,`sklearn.model_selection`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/German%20Traffic%20Signs/Using%20CNN%20to%20classify%20German%20Traffic%20Signs.ipynb)|
|[Using Computer Vision to classify Plant Seedlings](#Using-Computer-Vision-to-classify-Plant-Seedlings)|`Convolutional Neural Networks` | `tensorflow.keras`, `tensorflow.keras.preprocessing.image`,`sklearn.model_selection`|[Click](https://github.com/debajyotid/Using-Computer-Vision-to-classify-Plant-Seedlings/blob/main/Using%20Computer%20Vision%20to%20classify%20Plant%20Seedlings.ipynb)|
|[FashionMNIST and CIFAR10 Classification using CNN](#Using-Convolutional-Neural-Networks-for-classifying-FashionMNIST-and-CIFAR10-dataset)|`Convolutional Neural Networks` | `tensorflow.keras`, `tensorflow.keras.preprocessing.image`,`sklearn.model_selection`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/CNN_on_FashionMNIST_CIFAR10_with_ImageAugmentation.ipynb)|
|[Using DNN to classify images in Fashion-MNIST Dataset](#Using-Dense-Neural-Networks-for-classifying-FashionMNIST-dataset)|`Neural Networks` | `tensorflow.keras`, `tensorflow.keras.preprocessing.image`,`sklearn.model_selection`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/FashionMNIST_Classification.ipynb)|
|[Handwritten Digit Classification](#Using-Dense-Neural-Networks-for-Street-View-House-Numbers-Identification)|`Neural Networks` | `tensorflow.keras`, `sklearn.preprocessing`,`sklearn.model_selection`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/Handwritten%20Digit%20Classification/Predicting_hadnwritten_digits_using_DNN.ipynb)|
|[Predicting Customer Churn using Neural Networks](#Predicting-Customer-Churn-using-Neural-Networks)|`Neural Networks` |`imblearn`, `tensorflow.keras`, `sklearn.preprocessing`,`sklearn.model_selection`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/Predicting%20Customer%20Churn%20using%20ANN/Predicting_customer_churn_using_ANN.ipynb)|
|[Recommending Electronic Items using Collaborative Filtering](#Recommending-Electronic-Items-using-User-based-and-Item-based-Collaborative-Filtering)|`Recommendation Systems` |`surprise`, `SVD`, `collections`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Recommendation%20Systems/Recommending%20Electronic%20Items%20using%20Collaborative%20Filtering/CollaborativeFiltering_and_PopularityBased_RecommendationSystems.ipynb)|
|[Book Recommendation using Collaborative Filtering](#Book-Recommendation-using-User-based-Collaborative-Filtering)|`Recommendation Systems` |`surprise`, `SVD`, `collections`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Recommendation%20Systems/Book%20Recommendation%20using%20Collaborative%20Filtering/Recommendations_using_CollaborativeFiltering.ipynb)|
|[Predicting Loan Defualt using Randomforest Classifier](#Predicting-Loan-Defualt-using-Randomforest-Classifier)|`Ensemble Techniques` |`KFold`, `sklearn.utils`, `RandomForestClassifier`, `sklearn.preprocessing`, `sklearn.preprocessing`, `sklearn.model_selection`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Ensemble-Techniques/Predicting%20Loan%20Defualt%20using%20Randomforest%20Classifier/Predicting_LoanDefault_using_RandomForestClassifier.ipynb)|
|[Predicting onset of Parkinsons disease by analyzing voice sample using Ensemble Techniques](#Predicting-onset-of-Parkinsons-disease-by-analyzing-voice-sample-using-Ensemble-Techniques)|`Ensemble Techniques` |`DecisionTreeClassifier`, `RandomForestClassifier`, `PCA`, `sklearn.preprocessing`, `GridSearchCV`, `RandomizedSearchCV`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Ensemble-Techniques/Using%20Ensembles%20to%20predict%20Parkinsons%20Disease%20onset/Using%20Ensembles%20to%20predict%20Parkinsons%20Disease%20onset.ipynb)|
|[Classifying vehicles by analysing silhouettes](#Classifying-vehicles-by-analysing-their-silhouettes)|`Unsupervised Learning` |`StandardScaler`, `SVM`, `PCA`, `GridSearchCV`, `Cross-Val`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Unsupervised%20Learning/Classifying%20vehicles%20by%20analysing%20silhouettes/Vehicle_Classification_analyzing_silhouettes.ipynb)|
|[Predicting mileage for city vehicles using Cluster Analysis](#Cluster-Analysis-on-Vehicle-data-for-better-prediction-of-mpg-figures-for-each-class-of-vehicle)|`Unsupervised Learning` |`KMeans`, `AgglomerativeClustering`, `LinearRegression`, `GridSearchCV`, `sklearn.preprocessing`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Unsupervised%20Learning/Predicting%20mileage%20for%20city%20vehicles%20using%20Cluster%20Analysis/Predicting_mileage_using_Cluster_Analysis.ipynb)|
|[Campaign to sell Personal Loans](#Using-Supervised-Machine-Learning-techniques-to-create-a-successful-targetted-Perosnal-Loan-Campaign)|`Supervised Learning` |`LogisticRegression`, `KNeighborsClassifier`, `train_test_split`, `GridSearchCV`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Supervised-Learning/Campaign%20for%20selling%20Personal%20Loans/Campaign_for_Personal_Loans.ipynb)|
|[Classifying patients based on their orthopdeic biomechanical features](#Patient-Classification-using-orthopaedic-biomechanical-features)|`Supervised Learning` |`KNeighborsClassifier`, `MinMaxScaler`, `train_test_split`, `metrics`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Supervised-Learning/Patient%20Classification%20using%20biomechanical%20features%20of%20orthopaedic%20patients/Classifying_Patients_using_biomechanical_features.ipynb)|
|[Building a Student Performance Prediction System](#Building-a-Student-performance-prediction-system-using-Regression-techniques)|`Supervised Learning` |`LogisticRegression`, `GaussianNB`, `train_test_split`, `seaborn`, `labelencoder`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Supervised-Learning/Building%20a%20Student%20Performace%20Prediction%20System/Building_Student_Performace_Prediction_System.ipynb)|
|[Analyzing Cost of Insurance using Statistical Techniques](#Analyzing-Insurance-Cost-using-Statistical-Techniques)|`Hypothesis Testing` |`t-tests`, `Students t-Test`, `EDA`, `Anova`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Statistical-Learning/Analyzing_Insurance_costs_using_Statistical_techniques.ipynb)|
|[Hypothesis Testing Questions](#Hypothesis-Testing-Questions)|`Hypothesis Testing` |`t-tests`, `ANOVA`, `Type-I & Type-II Errors`, `Chi-Squared Tests`|[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Statistical-Learning/Hypothesis_Testing_Questions.ipynb)|

### YT Video Summarization Translation
---
![Image](https://i.ytimg.com/vi/h32ixnEK-8w/hq720.jpg?sqp=-oaymwEhCK4FEIIDSFryq4qpAxMIARUAAAAAGAElAADIQj0AgKJD&rs=AOn4CLC5oP85QZCV8wweD-XxfYAt0hU8gg)

In this repo we aim to get a concise YouTube video summary, review it to determine whether it's worth watching, extract step-by-step guidance so you could easily follow along, and at the end, generate a quiz to test your understanding.

Relevant jupyter notebook, with steps for extracting YouTube video transcripts, generating summaries from the same using pre-trained models from HuggingFace and finally experiment with ChatGPT APIs and leverage the ChatGPT APIs to generate a step-by-step guide from a YouTube video, can be found in the below repo. 

#### Repo Link
[Click](https://github.com/debajyotid/Generating_YT_Video_Summary_and_Quiz_sessions_from_Transcripts)

#### Skills and Tools 
Google Colab,OpenAI API,HuggingFace,Transformers,ChatGPT gpt-40-mini

### ML Model deployment on cloud
---
![Image](https://miro.medium.com/max/960/1*eUtgsrrFHwsV3wiD0te3jg.jpeg)

The purpose of this repository is to demonstrate different model deployment techniques like using Flask, StrealMit, FastAPI and on cloud environments like Heroku, AWS EC2, Azure.

The underlying ML models have been kept simple for demonstration purposes, but all the repositories have been provided with clear documentation to explain the different steps needed to successfully deploy ML models using the aforesaid technologies, either locally or on the cloud.

I am thanful to Krish Naik for sharing his repository and explaining in details these techniques in the below playlist:

[Deployment of ML Models](https://www.youtube.com/playlist?list=PLZoTAELRMXVOAvUbePX1lTdxQR8EY35Z1)

#### Repo Link
[Heroku-Demo](https://github.com/debajyotid/Heroku-Demo)

[Model-Deployment-using-AWS-EC2](https://github.com/debajyotid/Model-Deployment-using-AWS-EC2)

[Model-Deployment-using-Azure](https://github.com/debajyotid/Model-Deployment-using-Azure)

[Model-Deployment-using-Postman](https://github.com/debajyotid/Bank-Note-Authentication-model-deployment-using-Postman)

[Model-Deployment-using-Flasgger](https://github.com/debajyotid/Bank-Note-Authentication-model-deployment-using-Flasgger)

[Model-Deployment-using-Streamlit](https://github.com/debajyotid/Bank-Note-Authentication-model-deployment-using-StreamLit)

[Model-Deployment-using-Streamlit-on-Heroku](https://github.com/debajyotid/Bank-Note-Authentication-model-deployment-using-StreamLit-on-Heroku)

[Model-Deployment-using-FastAPI](https://github.com/debajyotid/Bank-Note-Authentication-model-deployment-using-FastAPI)

[Model-Deployment-using-FastAPI-on-Heroku](https://github.com/debajyotid/Bank-Note-Authentication-model-deployment-using-FastAPI-on-Heroku)

#### Skills and Tools 
Heroku,AWS EC2,Azure,Postman,StreamLit,FastAPI

---
### Using CNN to classify Dog Breeds based on images
---
![Image](https://user-images.githubusercontent.com/25509152/33898274-fa96ef88-df78-11e7-8f4c-59105584fdce.png)

The purpose of this repository is to identify & classify the breed of a dog, based on the subtle changes in its visual appearance. The idea is to explore advanced computer vision techniques to intelligently & correctly predict the breed of a dog. The data-set used for this project was hosted by Kaggle, as part of their Kaggle Playground Prediction Competition in 2017. [Click](https://www.kaggle.com/c/dog-breed-identification)

The data-set is a strictly canine subset of ImageNet & comprised of 120 breeds of dogs and a limited number training images per class. Each image was assigned a unique id and the same was tagged against the breed the dog in the image belonged to in a separate csv file.

We have tried 3 types of image resizing techniques before feeding to our models:

1.128X128X1
2.128X128X3
3.224X224X3

We have predominantly used 80:20 split for training & validation sets, but in the last iteration we have also tried to build the model on 90:10 split. We have throughout used only 10000 of the total 10222 records for training & validation.

We have evaluated 10 different types of models, details of which, along with other details like Evaluation techniques, metrices used, etc. can be found in the below repo. 

#### Repo Link
[Click](https://github.com/debajyotid/Using-CNN-to-classify-Dog-Breeds-based-on-images)

#### Skills and Tools 
CNN,VGG16,ResNet50,MobileNetV2,Image Augmentation,Transfer Learning

---
### Signature Verification using Siamese Networks 
---

#### Problem Statement:
---
Signature is one of the most popular and commonly accepted biometric hallmarks that has been used since the ancient times for verifying different entities related to human beings, viz. documents, forms, bank checks, individuals, etc. Therefore, signature verification is a critical task and many efforts have been made to remove the uncertainty involved in the manual authentication procedure, which makes signature verification an important research line in the field of machine learning and pattern recognition.

In this notebook, we model a writer independent signature verification task with a convolutional Siamese network.

#### About the Dataset:
---
The BHSig260 signature dataset contains the signatures of 260 persons, among them 100 were signed in Bengali and 160 are signed in Hindi.

For each of the signers, 24 genuine and 30 forged signatures are available. This results in:

100 × 24 = 2400 genuine and 100 × 30 = 3000 forged signatures in Bengali, and 
160 × 24 = 3840 genuine and 160×30 = 4800 forged signatures in Hindi.

In this task we are considering only Hindi singatures for easeness.

Paper Link: https://arxiv.org/pdf/1707.02131.pdf

#### Repo Link:
[Click](https://github.com/debajyotid/Signature-Verification-using-Siamese-Networks)

#### Skills and Tools
CNN,Siamese Network,Contrastive Loss,Model Checkpoint,Signature Verification

---
### Using Glove and LSTMs to classify Fake News
---
![Image](https://specials-images.forbesimg.com/imageserve/5f8b53d83ad376bd758e6b23/960x0.jpg?fit=scale)

---
Kaggle Link: | https://www.kaggle.com/c/fake-news

Aim is to develop a machine learning program to identify when an article might be fake news.
Run by the UTK Machine Learning Club.

---
Evaluation Metric
The evaluation metric for this competition is accuracy, a very straightforward metric.
[ accuracy = \frac{correct\ predictions}{correct\ predictions+incorrect\ predictions}]
Accuracy measures false positives and false negeatives equally, and really should only be used in simple cases and when classes are of generally equal class size

---
Data Description
train.csv: A full training dataset with the following attributes:
id: unique id for a news article title: the title of a news article author: author of the news article text: the text of the article; could be incomplete label: a label that marks the article as potentially unreliable
1: unreliable
0: reliable
test.csv: A testing training dataset with all the same attributes at train.csv without the label.
submit.csv: A sample submission

#### Repo Link:
[Click](https://github.com/debajyotid/Using-Glove-and-LSTMs-to-classify-Fake-News/blob/main/NLP-Kaggle%20Fake%20News%20Classifier%20using%20Glove%20%26%20LSTM.ipynb)

#### Skills and Tools
Sequential NLP,LSTM,Glove Word Embeddings,Keras Tokenization,Kaggle,Stemming

---
### Alligned Face Dataset Pinterest
---
## Face Recognition
In this hands-on project, the goal is to build a face identification model to recognize faces. 
The model uses Aligned Face Dataset from Pinterest. This dataset contains 10770 images for 100 people. All images are taken from 'Pinterest' and aligned using dlib library.

## Overview
In this problem, we use a pre-trained model trained on Face recognition to recognize similar faces.
Here, we are particularly interested in recognizing whether two given faces are of the same person or not. Below are the steps performed in the project.

● Load the dataset and create the metadata.

● Check some samples of metadata.

● Load the pre-trained model and weights.

● Generate Embedding vectors for each face in the dataset.

● Build distance metrics for identifying the distance between two given images.

● Use PCA for dimensionality reduction.

● Build an SVM classifier to map each image to its right person.

● Predict using the SVM model.

#### Repo Link
[Click](https://github.com/debajyotid/Creating-a-Face-Recognition-Model-using-a-VGG-Architecture-and-pretrained-weights/blob/main/Creating%20a%20Face%20Recognition%20Model%20using%20a%20VGG%20Architecture%20and%20pretrained%20weights.ipynb)

#### Skills and Tools
Face Recognition, VGG,SVM,Embeddings,Similarity Distance,OpenCV,PCA,StandardScaler,LabelEncoder

---
### WIDER Face Dataset
---
## Using Mobilenet in UNet Architecture to predict Face Masks

WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. We choose 32,203 images and label 393,703 faces with a high degree of variability in scale, pose and occlusion as depicted in the sample images. WIDER FACE dataset is organized based on 61 event classes. 

In this project, we are using 409 images and around 1000 faces for ease of computation.

We will be using transfer learning on an already trained model. We will use the MobileNet model which is already trained to detect the face attributes. We will need to train the last 6-7 layers and freeze the remaining layers to train the model for predicting the mask on the face. We use the MobileNet as the building block for a U-Net Architecture.

#### Repo Link
[Click](https://github.com/debajyotid/Using-MobileNet-in-UNet-Architecture-to-build-a-Face-Mask-Prediction-Model/blob/main/Using%20MobileNet%20in%20UNet%20Architecture%20to%20build%20a%20Face%20Mask%20Prediction%20Model.ipynb)

#### Skills and Tools 
UNet,MobileNet,CV2,Dice-Coefficient,Callbacks,ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,Learning Plots

---
### Kaggle Blog Authoship Corpus Challenge
---
## Using Statistical NLP to predict author of Blog

The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.

Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)

All bloggers included in the corpus fall into one of three age groups:

8240 "10s" blogs (ages 13-17),
8086 "20s" blogs(ages 23-27)
2994 "30s" blogs (ages 33-47).
For each age group there are an equal number of male and female bloggers.

Each blog in the corpus includes at least 200 occurrences of common English words.

This dataset contains information on writers demographics, including their age, gender and zodiac sign. Can we build a classifier to guess someone’s zodiac sign from blog posts they’ve written?

#### Repo Link
[Click](https://github.com/debajyotid/Great-Lakes/blob/master/Deep%20Learning/Using%20Statistical%20NLP%20to%20predict%20author%20of%20Blog.ipynb)

#### Skills and Tools 
NLTK,WordNetLemmatizer,CountVectorizer,OneVsRestClassifier,LogisticRegression,MultiLabelBinarizer

---
### Fake vs Real Image Detection Challenge
---
Great Lakes AIML Hackathon Challenge to detect Fake Images generated by GANs from authentic images.
---
This solution was adjudged the winner of 'Hack of All Trades' Hackathon conducted by Great Learning on 7-8th November 2020. 

The repository contains 2 solutions:
- A custom solution designed by us, which involved Image Augmentation, multiple callbacks & a custom CNN comprising of multiple Convolution Layers, Maxpooling Layers, Dropout & Batch Normalization; with ReLU activation function & ADAM optimizer. This file is labelled: Fake_vs_Real_Image_Detection_Challenge_withCustomCNN
- A Transfer Learning architecture using Resnet50 (without the Fully-Connected Layers) & custom Fully-Connected Layers with Dropout & Batch Normalization; with ReLU activation function & ADAM optimizer. This too has Image Augmentation, alog with Resnet50 specific Image Preprocessing & multiple callbacks. This file is labelled: Fake_vs_Real_Image_Detection_Challenge_withResNet50.

## Background:
---
With the rise of electronic media in the form of social media primarily, there has been a lot of misinformation spread in the form of images which are artificially created using modern augmentation tools. In order to segregate legitimate information from artificially created images, it is important to use modern technological advances to stop the spread of misinformation.
The Fake vs Real Image Detection Challenge draws inspiration from the above mentioned misrepresentation of data and tries to solve this problem using computer vision techniques.
Fake and Real images have been provided and we need to build a model that can segregate test images into Fake and Real Images. A mobile application or a software integrated into the mobile’s operating system can be built using this technology which can segregate fake images from real ones.

#### Repo Link
[Click](https://github.com/debajyotid/Fake-vs-Real-Image-Detection-Challenge)

#### Skills and Tools 
Resnet50,Computer Vision,Image Augmentation

---
### Predict Bike Sharing Demand using AWS SageMaker
---
![Image](https://miro.medium.com/max/800/1*yvJHtU3WgsZAtjv33iH3gA.jpeg)

The purpose of this repository is to develop a ML model to predict Bike Sharing demand as per the Kaggle competition [Click](https://www.kaggle.com/c/bike-sharing-demand/data)

The Kaggel data is downloaded locally & then uploaded to AWS SageMaker Notebook instance & the data preprocessing, model training & validation, and finally prediction is done using 3 seperate Jupyter notebooks. The Notebook Instance is integrated with this GitHub repository, so all changes committed in AWS is pushed to this repo. The aim is not to achieve a highly successful model, but rather use the features of AWS SageMaker & make a cloud-deployment use-case

#### Repo Link
[Click](https://github.com/debajyotid/SageMaker-Bike-Sharing)

#### Skills and Tools 
SageMaker,XGBoost,S3 Buckets

---
### Sentiment Analysis on Movie Reviews Classify the sentiment of sentences from the Rotten Tomatoes dataset
---
![Image](https://www.rottentomatoes.com/assets/pizza-pie/head-assets/images/RT_TwitterCard_2018.jpg)

This exercise uses the data from Kaggle's Sentiment Analysis competition. [Click](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data).

The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1].

In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus.

This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.

#### Skills and Tools
CountVectorizer,Tf-Idf Vectorizer,Glove-Word Embeddings,Word2Vec,Neural Networks,matplotlib.pyplot

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Rotten-Tomatoes-review-Sentiment-Analysis/blob/main/Rotten%20Tomatoes%20review%20Sentiment%20Analysis.ipynb)

---
### Bag of Words Meets Bags of Popcorn Using Google Word2Vec for movie reviews
---
![Image](https://m.media-amazon.com/images/G/01/imdbpro/help/TVlanding._CB1564536689_.PNG)

This exercise uses the data from Kaggle's IMDB Movie reviews competition [Click](https://www.kaggle.com/c/word2vec-nlp-tutorial/data).

In this tutorial competition, we dig a little "deeper" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.

Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. In this tutorial we explore how Word2Vec can be applied to a similar problem.

#### Skills and Tools
CountVectorizer,Tf-Idf Vectorizer,Glove-Word Embeddings,Word2Vec,Neural Networks,matplotlib.pyplot

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/IMDB-Reviews-Sentiment-Analysis-using-NLP-Technques/blob/main/IMDB%20Reviews%20Sentiment%20Analysis%20using%20NLP%20Technques.ipynb)

---
### Determining the preparedness of the German Government and Medical Authorities in handling COVID19 Crisis
---
![Image](https://taj-strategie.fr/content/uploads/2020/03/germany-coronavirus.png)

Purpose: 
In this paper, we try and analyse the trends in the spread of COVID-19 in the German population. We look into the statistical significance of the lockdown enforced by the German authorities in curbing the spread of the virus across the German population. We also try to develop a forecasting model to predict the number of infected patients & fatalities arising out of the infection. 

Materials & Methods: 
As part of this study, we obtained the dataset from 2 sources, namely Kaggle & Institute of Health Metrics & Evaluation (IHME). The datasets had data like number of cases, age-group of patient, number of available beds, number of tests done on a given day across the 2 datasets & the datasets were merged based different criterion like State, County, Date of Infection,etc. The dataset had data from 24-Jan-2020 up till 3-June-2020, and the same was obtained from across the 16 German states & their individual counties. 

Results: 
Statistical methods like t-Test & ANOVA resulted in extremely low p-values, based on which we can say with 99% confidence level that the different measures undertaken by German authorities, especially the imposition of a nationwide lockdown, had a statistically significant impact on controlling the spread of the SARS-CoV-2 virus in Germany. Visually we saw that the SARS-CoV-2 virus spread rose significantly in the pre-lockdown period (which we have assumed to be till 07-April-2020) and then started slowly tapering off. This also helped avoid severe stress on the medical institutions & we once again saw visually that the hospital admission rate was always below the rate at which new hospital beds were being added daily. This was also vindicated in our time-series forecasting model using Prophet, an open-source forecasting library from Facebook. 

Conclusion: 
Based on the above, we could statistically prove that the timely measures and steps taken by German authorities, like imposition of the lockdown, helped in both controlling the spread of COVID-19 across Germany, as well as kept the rate of fatalities due to the same at a relatively low rate.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Mini%20Capstone/Covid-19%20Germany%20(Data%20Pre%20Processing%20%26%20Hypothesis%20Testing)_Final%20version.ipynb)

#### Skills and Tools
Statistics, EDA, Visualization

---
### Understanding the performance of LightGBM and XGBoost
---
XGBoost is an open-source software library which provides a gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala. It works on Linux, Windows, and macOS. From the project description, it aims to provide a "Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, and Apache Flink. It has gained much popularity and attention recently as the algorithm of choice for many winning teams of machine learning competitions.

XGBoost initially started as a research project by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC) group. It became well known in the ML competition circles after its use in the winning solution of the Higgs Machine Learning Challenge. Soon after, the Python and R packages were built, and XGBoost now has package implementations for Java, Scala, Julia, Perl, and other languages. This brought the library to more developers and contributed to its popularity among the Kaggle community, where it has been used for a large number of competitions.

It was soon integrated with a number of other packages making it easier to use in their respective communities. It has now been integrated with scikit-learn for Python users.

Salient features of XGBoost which make it different from other gradient boosting algorithms include:

- Clever penalization of trees
- A proportional shrinking of leaf nodes
- Newton Boosting
- Extra randomization parameter
- Implementation on single, distributed systems and out-of-core computation


LightGBM, short for Light Gradient Boosting Machine, is a free and open source distributed gradient boosting framework for machine learning originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.

The LightGBM framework supports different algorithms including GBT, GBDT, GBRT, GBM, MART and RF. LightGBM has many of XGBoost's advantages, including sparse optimization, parallel training, multiple loss functions, regularization, bagging, and early stopping. A major difference between the two lies in the construction of trees. LightGBM does not grow a tree level-wise — row by row — as most other implementations do. Instead it grows trees leaf-wise. It chooses the leaf it believes will yield the largest decrease in loss.

#### Repo Link
[Click](https://github.com/debajyotid/Understanding-the-performance-of-LightGBM-and-XGBoost/blob/main/Understanding%20the%20performance%20of%20LightGBM%20and%20XGBoost.ipynb)

#### Skills and Tools
LabelBinarizer, XGBoost, LightGBM, PCA

---
### Understanding impact of Imblearn and PCA
---
An imbalanced classification problem is an example of a classification problem where the distribution of examples across the known classes is biased or skewed. The distribution can vary from a slight bias to a severe imbalance where there is one example in the minority class for hundreds, thousands, or millions of examples in the majority class or classes.

Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.

In this notebook we look to understand how we can solve this problem using different techniques like SMOTE, TomekLinks, RandomUnderSampler, etc. We also try to implemet PCA to reduce the number of collinear features, so as to avoid the problem of multi-collinearity & also arrive at a simpler model; without having too much of a negative impact on the predictive ability of the model.

#### Repo Link
[Click](https://github.com/debajyotid/Understanding-impact-of-Imblearn-and-PCA/blob/main/Understanding%20impact%20of%20Imblearn%20and%20PCA.ipynb)

#### Skills and Tools
LabelBinarizer, RandomForestClassifier, TomekLinks, SMOTE, RandomUnderSampler, Pipeline, PCA

---
### Predicting Salary Range using ML Techniques
---
![Image](http://www.salaryexplorer.com/charts/united-states/california/san-francisco/median-and-salary-distribution-yearly-san-francisco.jpg)

Problem Statement: Predict the pay-range a given employee would belong to, based on the analysis of different attributes of other employees & their respective pay-range.

The dataset is sourced from http://openbook.sfgov.org, which is part of the Government of San Francisco city's initiative in providing open & easily accessible data related to performance & spending across different departments of San Francisco City government. Salary range is the range of pay established by employers to pay to employees performing a particular job or function. Salary range generally has a minimum pay rate, a maximum pay rate, and a series of mid-range opportunities for pay increases.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Predicting-Salary-Range-using-ML-Techniques/blob/master/Predicting_SalaryRange_using_MLTechniques.ipynb)

#### Skills and Tools
LabelBinarizer, RandomForestClassifier, XGBClassifier, StackingCVClassifier, SMOTE, RandomUnderSampler, Pipeline

---
### Using ML techniques to predict GreatStone Ratings for MutualFunds
---
![Image](https://www.jagoinvestor.com/wp-content/uploads/files/Mutual-fund-types.jpg)

A mutual fund is a professionally managed investment fund that pools money from many investors to purchase securities. These investors may be retail or institutional in nature. Mutual fund ratings is one of the most influential and prominent decision making factors that is used by investors in making a
decision with regards to selecting a mutual fund. Great Stone Rating is a star based ranking system. These ratings are based on the performance of a mutual
fund with adjustments for risks and costs as compared to other funds in the same category. The rating ranges from
0 to 5.

Goal: The goal of this project is to predict GreatStone’s rating of a mutual fund. In order to help investors decide on which mutual fund to pick for an investment, the task is to build a model that can predict the rating of a mutual fund. The various attributes that define a mutual fund can be used for building the model. This dataset comprises information of 25000 mutual funds in the United states. Various attributes related to the mutual fund have been described and these attributes will be used for making decisions on the rating of the mutual fund by GreatStone which is a top mutual fund rating agency.

Dataset Information
Files Description:
The following files are provided in the form of CSVs. These files contain various attributes related to the mutual fund.
bond_ratings, fund_allocations, fund_config, fund_ratios, fund_specs, other_specs, return_3year, return_5year, return_10year. 

bond_ratings consists of 12 columns which provide information on the bond rating percentage allocation of the mutual funds.

fund_allocations consists of 12 columns which provide information on the sector wise percentage allocation of the mutual funds

fund_config comprises of 4 columns which comprise the metadata of the mutual funds

fund_ratios consists of 8 columns which provides information on various fundamental ratios that describe the mutual funds

fund_specs contains 9 columns which give information about the specifications of the mutual funds

other_specs contains 43 columns which give information of the other aspects of the mutual funds

return_3years contains 17 columns which give information about 3 year return and ratios

return_5years contains 17 columns which give information about 3 year return and ratios

return_10years contains 17 columns which give information about 3 year return and ratios

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Predicting-Mutual-Fund-Ratings/blob/master/Predicting_MutualFund_Ratings_using_ML_techniques.ipynb)

#### Skills and Tools
LabelBinarizer, RandomForestClassifier, XGBClassifier, StackingCVClassifier, SMOTE, RandomUnderSampler, Pipeline

---
### Using CNN to classify German Traffic Signs
---
![Image](https://pbs.twimg.com/media/DPdfoulVAAAoq86?format=jpg&name=medium)

German Traffic Sign Recognition
Multi-class, single-image classification

Dataset
The German Traffic Sign Benchmark is a multi-class, single-image classification challenge held at the International Joint Conference on Neural Networks (IJCNN) 2011. They cordially invite researchers from relevant fields to participate: The competition is designed to allow for participation without special domain knowledge. Their benchmark has the following properties:
Single-image, multi-class classification problem
More than 40 classes
More than 50,000 images in total
Large, lifelike database

The details about the full GTSRB dataset and the results of the final competition that was held at IJCNN 2011 can be found in the paper "Man vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition" that was accepted for publication in a Neural Networks Special Issue.

The paper is currently in print, but it is already available as online version here:
http://dx.doi.org/10.1016/j.neunet.2012.02.016

Preliminary citation:

J. Stallkamp, M. Schlipsing, J. Salmen, C. Igel, Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition, Neural Networks, Available online 20 February 2012, ISSN 0893-6080, 10.1016/j.neunet.2012.02.016. 
(http://www.sciencedirect.com/science/article/pii/S0893608012000457) 
Keywords: Traffic sign recognition; Machine learning; Convolutional neural networks; Benchmarking
http://benchmark.ini.rub.de/

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/German%20Traffic%20Signs/Using%20CNN%20to%20classify%20German%20Traffic%20Signs.ipynb)|

#### Skills and Tools
Convolution, Maxpooling, Tensorflow, Keras, Dropout, BatchNormalization, Softmax, Adam, Image Augmentation

---
### Using-Computer-Vision-to-classify-Plant-Seedlings
---
![Image](https://miro.medium.com/max/3868/1*KLmWe8SDXjqKzpd_qMomzw.png)

This dataset gives us an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas. The ability to do so effectively can mean better crop yields and better stewardship of the environment. Can we differentiate a weed from a crop seedling? Given an image, how do we differentiate between different plant types?

The Aarhus University Signal Processing group, in collaboration with the University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.

Here we are provided with a training set and a test set of images of plant seedlings at various stages of growing. Each image has a filename that is its unique id. The dataset comprises 12 plant species. The objective is to create a classifier capable of determining a plant's species from a photo

#### Repo Link
[Click](https://github.com/debajyotid/Using-Computer-Vision-to-classify-Plant-Seedlings/blob/main/Using%20Computer%20Vision%20to%20classify%20Plant%20Seedlings.ipynb)|

#### Skills and Tools
tensorflow.keras, tensorflow.keras.preprocessing.image,sklearn.model_selection

---
### Using Convolutional Neural Networks for classifying FashionMNIST and CIFAR10 dataset
---
![Image](https://aiinpractice.com/wp-content/uploads/2018/05/cifar10.jpg)

Correctly classify the different items in the FashionMNIST & CIFAR10 dataset using Convolutional Neural Networks. Additionally use Image augmentation by utilizing the ImageDataGenerator library.

Repo Link: [Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/CNN_on_FashionMNIST_CIFAR10_with_ImageAugmentation.ipynb)

#### Skills and Tools
Convolution, Maxpooling, Tensorflow, Keras, Dropout, BatchNormalization, Softmax, Adam, Image Augmentation

### Using Dense Neural Networks for classifying FashionMNIST dataset
---
![Image](https://peltarion.com/static/fashion-mnist_long.png)

Correctly classify the different items in the FashionMNIST dataset using Dense Neural Networks

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/FashionMNIST_Classification.ipynb)

#### Skills and Tools
Tensorflow, Keras, Dropout, BatchNormalization, Softmax, Adam

---
### Using Dense Neural Networks for Street View House Numbers Identification
---
![Image](https://research.cerenaut.ai/wp-content/uploads/2018/01/svhn-large.jpg)

In this project, we will use the dataset with images centered around a single digit (many of the images do contain some distractors at the sides). Although we are taking a sample of the data which is simpler, it is more complex than MNIST because of the distractors.

The Street View House Numbers (SVHN) Dataset SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with the minimal requirement on data formatting but comes from a significantly harder, unsolved, real-world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. 

Link to the dataset: https://drive.google.com/file/d/1L2-WXzguhUsCArrFUc8EEkXcj33pahoS/view?usp=sharing
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng 
Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011.

The objective of the project is to learn how to implement a simple image classification pipeline based on a deep neural network. 

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/Handwritten%20Digit%20Classification/Predicting_hadnwritten_digits_using_DNN.ipynb)

#### Skills and Tools
Tensorflow, Keras, Dropout, BatchNormalization, Softmax, Adam

---
### Predicting Customer Churn using Neural Networks
---
![Image](https://document360.io/wp-content/uploads/2019/02/customer-churn-.jpg)

The case study is from an open source dataset from Kaggle.
Link to the Kaggle project site:
https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling

Objective of this exercise was to determine that given a Bank customer, can we build a classifier which can determine whether they will leave or not using Neural networks?

We first build a simple Dense Neural Network (DNN) with 1 i/p layer (100 Neurons), 1 hidden layer (50 Neurons) and 1 o/p layer. With the same we obtained a testing accuracy of 80%, and the model generalized well over both training & testing.

We then proceeded towards optimizing the model by changing the number of epochs & batch_size per epoch. The latter will have direct impact on whether or not the model will get stuck in a local minima or will be able to proceed to a gloabl minima, thereby both achieving a good accuracy & generalization
We started in this direction with the default batch_size of 32 & 1000 epochs. This resulted in an extremely over-fit model, which achieved a training accuracy of 100%, but failed miserably in testing by only achieving 81%. Moreover, it took a horrendous amount of time to finish training. So we changed the batch_size to 70 and reduced the epochs to 100, thereby achieving a training accuracy of 90%, but testing accuracy of only 85%.

We took our chances by increasing the batch_size further to 100 and reducing epochs to 70. We still hadn't reached a well-generalized model as our model was still doing 88-86, across training-testing respectively. However, our testing accuracy had now risen from 81-85-86.

So we decided to tinker further & check if a more generalized model could be designed, and proceeded with batch_size=700 & epochs=10. This caused our model to drop it's testing accuracy to 84%, but it was a closer approximation of our training accuracy of 85%. Thus we could say that we had achieved a more-or-less generalized model. As a batch_size of 700 represents 10% of a training dataset of 7000 datapoints, and with 10 epochs the model was also getting trained much faster, we decided to continue with this epoch & batch_size.

We lastly added an extra hidden layer of 50 neurons with activation function as RELU & saw that the model did slightly better at 86-85% acorss training-testing respectively. This was our final model.

In our initial model we had used the default RMSPROP optimizer & the tuned model we used RMSPROP with Nestrov Momentum, i.e. NADAM optimizer.

We didn't practise in imbalance learning optimization as we felt that in real-world too the class distribution might be similar. However, we ensured that the class distribution remained identical across both training & testing datasets.

We observed that the model was having a very poor RECALL score for customers who HAVE EXITED, which was actually our target customers, people who we wish to retain back. Thus the above model was defeating our basic requirement & decided to look at other options to improve our model. One such approach was SYNTHETIC OVER-SAMPLING & RANDOM UNDER-SAMPLING. Using the same we were able to design a model which is having comparable accuracy to our previous model while at the same time has a much higher PRECISION & RECALL for our target class.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Deep%20Learning/Predicting%20Customer%20Churn%20using%20ANN/Predicting_customer_churn_using_ANN.ipynb)

#### Skills and Tools
SMOTE, RandomUnderSampler, Pipeline, Tensorflow, Keras

---
### Recommending Electronic Items using User based and Item based Collaborative Filtering
---
Everyday a million products are being recommended to users based on popularity and other metrics on e-commerce websites. The most popular e-commerce website boosts average order value by 50%, increases revenues by 300%, and improves conversion. In addition to being a powerful tool for increasing revenues, product recommendations are so essential that customers now expect to see similar features on all other eCommerce sites.

First three columns are userId, productId, and ratings and the fourth column is timestamp. 
Source - Amazon Reviews data (http://jmcauley.ucsd.edu/data/amazon/).
The repository has several datasets. For this case study, we are using the Electronics dataset.

We first devised a POPULARITY BASED recommendation system wherein we were able to predict the top-5 most popular products to any new users. This doesn't require us to have any apriori knowledge of the users.

While we proceeded with the entire dataset while we were designing our POPULARITY BASED MODEL, it didn't make sense to take the entire dataset for our COLLABORATIVE FILTERING MODEL, especially when we saw that a huge number of users had rated only 1 item. This would have caused undue sparsity in our collaboration matrix & we thus filtered down our data to only those users who have rated at least 10 items

If we filtered our data further, by choosing only such users who have rated say 15 or more items, we could've achieved a better RMSE as our user-item collaboration matrix would be more dense & our model would be thus more accurate

However, going with only those users who rated at least 10 items, we were able to achieve a RMSE of 1.49 with a plain-vanilla SVD model, and once we tuned the same we were able to bring the RMSE to below 1
Using this model, we were then able to design a recommendation system which would be able to predict the top-5 products for a given user

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Recommendation%20Systems/Recommending%20Electronic%20Items%20using%20Collaborative%20Filtering/CollaborativeFiltering_and_PopularityBased_RecommendationSystems.ipynb)

#### Skills and Tools
User-User Collaborative Filtering, Item-Item Collaborative Filtering, SVD, KNNWithMeans

---
### Book Recommendation using User based Collaborative Filtering
---
![Image](https://www.researchgate.net/profile/Marwa_Mohamed49/publication/331063850/figure/fig3/AS:729493727621125@1550936266704/Content-based-filtering-and-Collaborative-filtering-recommendation.ppm)

The Objective of this project entails building a Book Recommender System for users based on user-based and item-based collaborative filtering approaches.

The dataset has been compiled by Cai-Nicolas Ziegler in 2004, and it comprises of three tables for users, books and ratings. Explicit ratings are expressed on a scale from 1-10 (higher values denoting higher appreciation) and implicit rating is expressed by 0.
Reference: http://www2.informatik.uni-freiburg.de/~cziegler/BX/


#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Recommendation%20Systems/Book%20Recommendation%20using%20Collaborative%20Filtering/Recommendations_using_CollaborativeFiltering.ipynb)

#### Skills and Tools
User-User Collaborative Filtering, SVD

---
### Predicting Loan Defualt using Randomforest Classifier
---
![Image](https://www.debt.org/wp-content/uploads/2012/07/Default-on-Loans.jpg)

Based on different attributes of a bank's customers, predict whether a customer will default or not, using different ML techniques like K-Fold Cross-Validation, RandomForestClassifier, etc. Also the model's performance is explained using a ROC-AUC plot

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Ensemble-Techniques/Predicting%20Loan%20Defualt%20using%20Randomforest%20Classifier/Predicting_LoanDefault_using_RandomForestClassifier.ipynb)

#### Skills and Tools
K-Fold Cross-Validation, RandomForestClassifier, GridSearchCV, LabelEncoder, OneHotEncoder, ROC-AUC

---
### Predicting onset of Parkinsons disease by analyzing voice sample using Ensemble Techniques
---
![Image](https://www.genengnews.com/wp-content/uploads/2019/06/203938_web.jpg)

Parkinson’s Disease (PD) is a degenerative neurological disorder marked by decreased dopamine levels in the brain. It manifests itself through a deterioration of movement, including the presence of tremors and stiffness. There is commonly a marked effect on speech, including dysarthria (difficulty articulating sounds), hypophonia (lowered volume), and monotone (reduced pitch range). Additionally, cognitive impairments and changes in mood can occur, and risk of dementia is increased.

Traditional diagnosis of Parkinson’s Disease involves a clinician taking a neurological history of the patient and observing motor skills in various situations. Since there is no definitive laboratory test to diagnose PD, diagnosis is often difficult, particularly in the early stages when motor effects are not yet severe.

Monitoring progression of the disease over time requires repeated clinic visits by the patient. An effective screening process, particularly one that doesn’t require a clinic visit, would be beneficial. Since PD patients exhibit characteristic vocal features, voice recordings are a useful and non-invasive tool for diagnosis. If machine learning algorithms could be applied to a voice recording dataset to accurately diagnosis PD, this would be an effective screening step prior to an appointment with a clinician.

The data & attributes information for this project is available at https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/ 
The data consists of those diagnosed with Parkinson Disease and those who do not.

Data Set Information:
This dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson's disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals ("name" column). The main aim of the data is to discriminate healthy people from those with PD, according to "status" column which is set to 0 for healthy and 1 for PD.

The data is in ASCII CSV format. The rows of the CSV file contain an instance corresponding to one voice recording. There are around six recordings per patient, the name of the patient is identified in the first column.

This dataset is courtesy the below & maybe copyrighted by the same.
Max A. Little, Patrick E. McSharry, Eric J. Hunter, Lorraine O. Ramig (2008), 'Suitability of dysphonia measurements for telemonitoring of Parkinson's disease', IEEE Transactions on Biomedical Engineering (to appear).

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Ensemble-Techniques/Using%20Ensembles%20to%20predict%20Parkinsons%20Disease%20onset/Using%20Ensembles%20to%20predict%20Parkinsons%20Disease%20onset.ipynb)

#### Skills and Tools
DecisionTreeClassifier, RandomForestClassifier, PCA, sklearn.preprocessing, GridSearchCV, RandomizedSearchCV

---
### Classifying vehicles by analysing their silhouettes
---
![Image](https://cdn-ds.com/blogs-media/sites/43/2019/03/01204421/black-car-silhouette-side-view_u-1038x375.jpg)

The purpose of the case study is to classify a given silhouette as one of three different types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles.

Four "Corgie" model vehicles were used for the experiment: a double-decker bus, Chevrolet van, Saab 9000 and an Opel Manta 400 cars.This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Unsupervised%20Learning/Classifying%20vehicles%20by%20analysing%20silhouettes/Vehicle_Classification_analyzing_silhouettes.ipynb)

#### Skills and Tools
K-fold Cross Validation, StandardScaler, SVM, GridSearchCV, PCA

### Cluster Analysis on Vehicle data for better prediction of mpg figures for each class of vehicle
---
![Image](https://static-ssl.businessinsider.com/image/5cc9c363021b4c10db31af65-2059/here%202.jpg)
The dataset was used in the 1983 American Statistical Association Exposition. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 2 multivalued discrete and 4 continuous variables.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Unsupervised%20Learning/Predicting%20mileage%20for%20city%20vehicles%20using%20Cluster%20Analysis/Predicting_mileage_using_Cluster_Analysis.ipynb)

#### Skills and Tools
KMeans, AgglomerativeClustering, LinearRegression, GridSearchCV

---
### Using Supervised Machine Learning techniques to create a successful targetted Perosnal Loan Campaign
---
This case is about a bank (Thera Bank) which has a growing customer base. Majority of these customers are liability customers (depositors) with varying size of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans.
In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).
A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.
The department wants to build a model that will help them identify the potential customers who have higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign.
The dataset is readily available in Kaggle & has also been included in the repo. The file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Supervised-Learning/Campaign%20for%20selling%20Personal%20Loans/Campaign_for_Personal_Loans.ipynb)

#### Skills and Tools
LogisticRegression, KNeighborsClassifier, train_test_split, GridSearchCV

---
### Patient Classification using orthopaedic biomechanical features
---
In this assignment we look to classify whether a patient has started noticing onset of Rheumatoid Arthritis based on the biomechanical features like pelvic_incidence, lumbar_lordosis_angle, pelvic_radius, etc. The dataset has 2 parts: 1 having 2 classes-Normal/Abnormal, while the other having 3 classes-Normal/Spondylolisthesis/Hernia. The dataset is part of UCI Machine Learning repository. We use a popular supervised machine learning algorithn KNeighborsClassifier for our classification task. This algorithm works by classifying a datapoint to a particular class based on the proximity of each indvidual feature in the datapoint with other datapoints. The datapoints is assigned the class of the nearest datapoint(s). Number of nearest neighbours to be used for this learning is an extremely important hyper-parameter as is the method used to calculate the distance of a point from it's nearest neighbours.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Supervised-Learning/Patient%20Classification%20using%20biomechanical%20features%20of%20orthopaedic%20patients/Classifying_Patients_using_biomechanical_features.ipynb)

#### Skills and Tools
KNeighborsClassifier, MinMaxScaler, train_test_split, metrics

---
### Building a Student performance prediction system using Regression techniques
---
The dataset consists of student achievements in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in Mathematics.
Source: https://archive.ics.uci.edu/ml/datasets/Student+Performance

We use LogisticRegression & Gaussian Naive-Baye's Classifiers for calculating the probability of a student passing & predicting the same based on the calculation

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Supervised-Learning/Building%20a%20Student%20Performace%20Prediction%20System/Building_Student_Performace_Prediction_System.ipynb)

#### Skills and Tools
LogisticRegression, GaussianNB, train_test_split, seaborn, labelencoder

---
### Analyzing Insurance Cost using Statistical Techniques
---
In the case of an insurance company, attributes of customers like Age, Gender, BMI, No. of Children, Smoking habits, etc. can be crucial in making business decisions. Hence, knowing to explore and generate value out of such data can be an invaluable skill to have.

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Statistical-Learning/Analyzing_Insurance_costs_using_Statistical_techniques.ipynb)

#### Skills and Tools
t-Test, Student's t-Test, ANOVA, EDA

### Hypothesis Testing Questions
---
In this assignment, we look at various statistical techniques like t-Tests, ANOVA, Chi-Square Tests, etc. and try answer various questions statistically using Hypothesis Testing

#### Repo Link
[Click](https://nbviewer.jupyter.org/github/debajyotid/Great-Lakes/blob/master/Machine%20Learning/Statistical-Learning/Hypothesis_Testing_Questions.ipynb)

#### Skills and Tools
t-Tests, ANOVA, Type-I & Type-II Errors, Chi-Squared Tests
